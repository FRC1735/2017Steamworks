// RobotBuilder Version: 2.0
//
// This file was generated by RobotBuilder. It contains sections of
// code that are automatically generated and assigned by robotbuilder.
// These sections will be updated in the future when you export to
// Java from RobotBuilder. Do not put any code or make any change in
// the blocks indicating autogenerated code or it will be lost on an
// update. Deleting the comments indicating the section will prevent
// it from being updated in the future.


package org.usfirst.frc1735.Steamworks2017.subsystems;

import org.usfirst.frc1735.Steamworks2017.RobotMap;
import org.usfirst.frc1735.Steamworks2017.commands.*;

import com.ctre.CANTalon;
import com.ctre.CANTalon.FeedbackDevice;
import com.ctre.CANTalon.TalonControlMode;

import edu.wpi.first.wpilibj.command.Subsystem;
import edu.wpi.first.wpilibj.networktables.NetworkTable;


/**
 *
 */
public class Turret extends Subsystem {

    // BEGIN AUTOGENERATED CODE, SOURCE=ROBOTBUILDER ID=CONSTANTS

    // END AUTOGENERATED CODE, SOURCE=ROBOTBUILDER ID=CONSTANTS

    // BEGIN AUTOGENERATED CODE, SOURCE=ROBOTBUILDER ID=DECLARATIONS
    private final CANTalon turretTurner = RobotMap.turretTurretTurner;

    // END AUTOGENERATED CODE, SOURCE=ROBOTBUILDER ID=DECLARATIONS

    // We need a constructor
	public Turret() {		
	    // Get a pointer to the networkTable.  "BoilerVision" is the name we entered into the publish box in GRIP
	    m_boilerTable = NetworkTable.getTable("GRIP/BoilerVision");
		}

    // Put methods for controlling this subsystem
    // here. Call these from Commands.

    public void initDefaultCommand() {
        // BEGIN AUTOGENERATED CODE, SOURCE=ROBOTBUILDER ID=DEFAULT_COMMAND


    // END AUTOGENERATED CODE, SOURCE=ROBOTBUILDER ID=DEFAULT_COMMAND

        // Set the default command for a subsystem here.
        // setDefaultCommand(new MySpecialCommand());
    }
    
    // Routine to initialize the run-time parameters for the Talon PID controller.
    // Example taken from CTRE github PositionClosedLoop
    // WE don't want to do this at construction time, so make RobotInit() call this function.
    public void turretInit() {
    	// Choose the sensor and sensor dirction
    	turretTurner.setFeedbackDevice(FeedbackDevice.AnalogEncoder);
    	turretTurner.reverseSensor(false); //Assume no inversion at this time.
    	turretTurner.configPotentiometerTurns(1); // Assume a 1:1 ratio of turret to encoder rotations
    	turretTurner.configNominalOutputVoltage(+0f,  -0f);
    	turretTurner.configPeakOutputVoltage(+12f,  -12f);
    	
    	/* set the allowable closed-loop error,
         * Closed-Loop output will be neutral within this range.
         * See Table in Section 17.2.1 for native units per rotation. 
         */
        turretTurner.setAllowableClosedLoopErr(0); /* always servo */
        /* set closed loop gains in slot0 */
        turretTurner.setProfile(0);
        turretTurner.setF(0.0);
        turretTurner.setP(0.1);
        turretTurner.setI(0.0); 
        turretTurner.setD(0.0);
        
        // Because we have limited motion in either direction, we may need to specify soft limits on the absolute encoder value.
        // The actual limit values will probably vary due to assembly (far left edge might not be exactly zero>)
        turretTurner.setForwardSoftLimit(m_forwardSoftLimit);
        turretTurner.enableForwardSoftLimit(true);
        turretTurner.setReverseSoftLimit(m_reverseSoftLimit);
        turretTurner.enableReverseSoftLimit(true);
        
        // Set the mode to be position-based
        turretTurner.changeControlMode(TalonControlMode.Position);
        // Must set an initial setpoint as well
        turretTurner.set((m_forwardSoftLimit+m_reverseSoftLimit)/2); // Center between the absolute left and right
	}    	
    	
    //-----------------------
    // operate
    //-----------------------
    // This function is the main command for the turret.
    // it is called by the subsystem defaultCommand every DS time loop (~20ms)
    // The algorithm is:
    // - Grab the latest position data from the vision coprocessor
    // - Calculate the amount we are "off" from being centered
    // - grab our current/absolute position
    // - calculate a new absolute position to get us centered based on the current offset
    // - Provide the new absolute position as the new setpoint to the PID
    
    // The PID itself runs in hardware on the TalonSRX and iterates at a rate much faster than the camera data refresh or the 20ms DS loop.
    // So, we need to be careful not to send stale camera data to the PID if it's already close to the actual goal based on the last "good" snapshot.
    // Not sure how to limit this unless we only get snapshots when we think we have gotten onTarget from the last snapshot
    public void operate() {
    	double targetAngle;
    	double currentSetpoint = turretTurner.getSetpoint();
    	// Are we actively targeting? And have we reached the last snapshot's target position?  if so, calculate a new target
    	if (this.isVisionEnabled() && this.onTarget()) {
        	// Query the camera info to get our angle of error from the target.
        	// If no target is currently visible, returns an error of zero (this prevents us from spinning around aimlessly if we lose the target)
        	// @FIXME:  Eventually, some kind of algorithm to help with lost vision target would be good-- do we actually search for it?
        	double errorAngle = getErrorAngle(); // Need to be careful about the sign.  Assume that lower numbers are to the left (relative to shooter).
        	double currentAngle = turretTurner.get()*360; // go relative to our current position (which might not QUITE be the setpoint)
        	targetAngle = currentAngle + errorAngle; // if error is negative we will turn to the left.
    	}
    	else {
    		// either we are not targeting, or we are in motion right now; keep the current setpoint that we defaulted to at the top of the function
        	targetAngle = currentSetpoint*360; // convert setpoint's "rotations" into "Degrees"
		}
    	
    	// Make the targetAngle the new setpoint (Must do this periodically to avoid Motor safety timeouts!)
    	// However, the PID takes ROTATIONS as the unit of measurement.
    	// So, if one rotation is 360 degrees, then we simply divide the angle by 360.
    	// @FIXME: Again, this assumes the left edge is at zero; may need a correction factor throughout this function...
    	turretTurner.set(targetAngle/360);
    }
    
    // Returns true if we are actively using the turret (enables vision snapshots and the HW PID controller)
    public boolean isVisionEnabled() {
    	return m_visionEnabled;
    }
    
    // Set the mode (active = true)
    public void setVisionEnable(boolean isEnabled) {
    	m_visionEnabled = isEnabled;
    }
        
    // Return if we think the PID is at its target setpoint right now
    public boolean onTarget() {
    	// Error is reported in (Fractional) rotations with one rotation being 1024 units.
    	// Therefore, if we want error of 1%, we want the reported error to be <= 10.24
    	if (Math.abs(turretTurner.getClosedLoopError()) <= 10.24) {
    		return true;
    	}
    	else {
    		return false;
    	}
    }
    
    //----------------------------------
    // getErrorAngle()
    //----------------------------------
    // This is the "meat" of the vision processing system.
    // Grab the current "centered X" position stuffed into the NetworkTable by the coprocessor,
    // and calculate how far off-center we are.
    public double getErrorAngle() {
    	// First, find out how far off center we are in the image.
    	// The desired centerline is Xres/2 + our compensation for camera vs shooter location
    	double desiredCenter = m_xRes/2 + m_targetCenterOffset;
    	
    	// Get the raw position and object width from NetworkTables:
    	double [] rawData = getRawTargetData();
    	double currentXPos = rawData[0];
    	double xWid = rawData[1];
    	
    	// could do this:
    	//double calculatedDistance = CalculateDistanceFromCamera(xWid);
    	//double angle =  Math.atan((currentXPos - desiredCenter) / calculatedDistance);
    	
    	// or calculate directly, independent of distance:
    	// Given theta = 26.565 from center to edge of FOV (empirically determined; need to recalculate for 2017)
    	// if the raw pixel distance over that angle is XRes/2=160, then each pixel off-center is 26.565/160 = 0.16603125 degrees
    	// Our answer is therefore is the pixel error multiplied by degrees_per_pixel:
    	double pixelError = currentXPos - desiredCenter;
    	return (pixelError * (26.565/(m_xRes/2)));
    	
    	
    }
    
    // Return data for a single contour (the Table presents an array of each piece of data, unfortunately... have to turn it sideways
    // [0]: the pixel position of the center of the target's X axis
    // [1]: the width of the target (total width)
    public double[] getRawTargetData() {
    	double xPos;
    	double xWid; // components of our return value
    	
    	// 1) Get the current list of targets found.  There might be more than one visible at a time if our processing is noisy,
    	//    or if we can't combine the two tape strips into a single blob on the coprocessor
       	// First step: get the vision system data for the target

    	// Get all needed table items at (roughly) the same time, to minimize table updates between reads.
    	// (could end up with different array sizes)
    	double[] defaultValue = new double[0]; // set up a default value in case the table isn't published yet
    	double[] targetX = m_boilerTable.getNumberArray("centerX", defaultValue);    	
		double[] width = m_boilerTable.getNumberArray("width", defaultValue);
		if (targetX.length != width.length) {
			// here the table updated in the middle; we'll have to punt.
			// (Yes, it could have updated to the same number of objects, but different objects.  There is no way to detect that at all)
			// This is just to indicate noise where the number of identified contours is varying rapidly, possibly between none and one.
			System.out.println("NetworkTable udpated in the middle of getRawTargetData; may have inconsistent datapoints!");
		}
		// Choose the first object, if one was found.
    	if (targetX.length==0) {
    		// We didn't find a valid x position to use.
    		// Return a perfectly centered answer so that the system doesn't try to adapt
    		// We also have to compensate for offset between the camera image and the actual robot.
    		xPos = ((m_xRes/2) + m_targetCenterOffset);
    	}
    	else {
    		xPos = targetX[0]; // Use the first Xposition value
    	}
    	if (width.length==0) {
    		// No valid width.  punt and set the width to some value that matches what we'd see in auto firing from the hopper...
    		xWid = 10; // This is an arbitrary choice until we measure it.
    	}
    	else {
    		xWid = width[0]; // Use the first width value
    	}
	    	
	    	
    	// For initial debug, just print out the table so we can see what's going on
/*
    	System.out.print("centerX: ");
    	for (double xval : targetX) { // for each target found,
    		System.out.print(xval + " ");
    	}
    	System.out.println();
*/    	
    	
	    // Return an array of the answers
	    double[] rawData = {xPos, xWid};
	    return rawData;
     }

    //@FIXME: empirically determined in 2016; need to recalculate most of this
    public double calculateDistanceFromCamera(double targetWidthPixels) {
    	//d = Tft*FOVpixel/(2*Tpixel*tan(theta))
    	double distance, targetWidthInches, FOVFeet, FOVPixels, theta;
    	theta = 26.565; 
    	targetWidthInches = 15; // Even though target tape is a circumference, to a camera it just appears as the diameter of the tube
    	//targetWidthPixels = 44.1379; // calculated from measured FOVpixel FOVfeet, and targetWidthFeet at d=12'.
    	//FOVFeet = 145/12; //FOVFeet measured empirically (in inches) at d=12'
    	FOVPixels = m_xRes;
    	
    	// If we don't see a target, the width will be zero.
    	// Punt by setting the distance to be something in the middle... 12' sounds good.
    	if (targetWidthPixels > 0)
    		//distance = (targetWidthInches*FOVPixels)/(2*targetWidthPixels*Math.tan(theta));
    		distance = 4800/targetWidthPixels;
    	else
    		distance = 5; //Arbitrary but legal value for "punting"...
    	return distance;
    }

    // Accessors for turret Commands like TurretWithJoystick
    public double getLeftLimit() { return m_reverseSoftLimit; }
    public double getRightLimit() { return m_forwardSoftLimit; }

    //----------------------------------
    // Member Variables
    //----------------------------------
    private boolean m_visionEnabled = false; // Is the turret active?
    private double m_reverseSoftLimit = 0.0;
    private double m_forwardSoftLimit = 85.3; // Assumes 1024 ticks/rev, 30' of motion, and left edge is at zero.
	NetworkTable m_boilerTable;
	public static final double m_xRes = 320; // this is the maximum resolution in pixels for the x (horizontal) direction-- determined by how the vision pipeline is implemented.
	// This is the offset (in PIXELS) that we need to compensate between the camera center and the robot shooting centered.
	// You can determine this empirically by getting the robot to shoot perfectly and then reading the raw Xpos from the vision system...
	private double m_targetCenterOffset = 0;    //(+1 means the robot is really centered when the image center is 1 pixel to the right of dead center xRes/2) 
}

